{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "from torch import cuda, nn, optim, concat\n",
    "from torch.backends import mps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering, HDBSCAN, MeanShift, estimate_bandwidth\n",
    "\n",
    "import datasets.mnist_loader as ml\n",
    "from ptmodels import vae_pytorch as vp\n",
    "from plotting import ae_plots\n",
    "\n",
    "# Use a gpu or M1 chipset to train PyTorch networks if you have it.\n",
    "if cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "elif mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "else: \n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using {device}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Autoencoders and Variational Autoencoders.\n",
    "In this project, we are going to first implement a couple autoencoders trained on MNIST-compatible data sets for dimensionality reduction. Afterwards, we'll then create a generative model using a variational autoencoder. \n",
    "\n",
    "## Project 2.1: Autoencoders for Dimensionality Reduction.\n",
    "High dimensionality data frequently has more dimensions than is needed to perform regression, classification, or clustering.  More formally, there is a lot of covariance within most data, covariance that reduces the intrinsic dimensionality of the data set.  Think of image data --- a $128 \\times 128$ pixel image can be thought of as a vector $\\mathbf{x}$ which resides in a $416384$-dimensional vector space. That does not mean there are $16384$ unique features.  Intuitively, we know there are less features in the data, and those features are captured by correlations between pixels.  In other words, we could find a mapping from the starting representation $\\mathbf{x}$ to a reduced-dimension latent representation $\\mathbf{z}\\in \\mathcal{R}^m$, where hopefully $m \\ll 16384$.  We can then use more approachable latent representation $\\mathbf{z}\\in \\mathcal{Z}$ to analyze the starting dataset $\\mathbf{x}\\in \\mathcal{X}$.\n",
    "\n",
    "An autoencoder does this by finding three things: a latent representation $\\mathbf{z}\\in \\mathcal{Z}$, an encoding function $E_{\\phi}(\\mathbf{x})=\\mathbf{z}$ parameterized by $\\phi$, and a decoding function $D_{\\theta}(\\mathbf{z})=\\mathbf{x}$ parameterized by $\\theta$. Here, we will simultaneously train two dense, multi-layer perceptrons to estimate functions $E_{\\phi}$ and $D_{\\theta}$, recovering the latent space $\\mathcal{Z}$ in the process. \n",
    "\n",
    "Training the perceptron networks requires a loss function.  Although the data we are training on is labeled, we will not be using them.  Instead, we are going to perform unsupervised learning.  Specifically, we'll are going to optimize by minimizing the 'distance' between the starting vector $\\mathbf{x}$ and its predicted decoding $D_{\\phi}(E_{\\theta}(\\mathbf{x}))$: \n",
    "\n",
    "$L(\\mathcal{X}|\\phi, \\theta)=-\\frac{1}{N}\\sum^N_{i=1}L_2(\\mathbf{x}_i, D_{\\phi}(E_{\\theta}(\\mathbf{x}_i)))$\n",
    "\n",
    "where $N$ is the size of the training data sample $\\mathcal{X}$ and $L_2(\\mathbf{x}, \\mathbf{x}^{\\prime})=|| \\mathbf{x} - \\mathbf{x}^{\\prime} ||^2$ is the L2 loss (basically, the Euclidian distance up to some multiplicative constant). To train, we will minimize $L(\\mathcal{X}|\\phi, \\theta)$ with respect to the parameters $\\phi$ and $\\theta$.\n",
    "\n",
    "Enough math.  Let's start setting up the model to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)\n",
    "# Let's load some PyTorch DataLoaders we'll be using using a wrapper function in the 'mnist_loader.py' module.\n",
    "train_dl, validation_dl = ml.load_MNISTlike(\n",
    "    target_set='MNIST', batch_size_train=64, batch_size_validation=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect an image\n",
    "x, y = train_dl.dataset.__getitem__(0)\n",
    "\n",
    "print(x[0].shape)  # each image is a single-channel 28 x 28 image.\n",
    "print(y)  # one-hot encoding correponds to the number 5 label. \n",
    "plt.imshow(x.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a loss function from Pytorch that satisfies the requirements above.  Luckily, `torch.nn.MLELoss` does just that. We'll also use the Adam optimizer.  As for the the model itself, the number of hidden layers will be four, including the final latent layer estimating $\\mathcal{Z}$. The starting input dimensionality is $28 \\times 28=784$ and we are going to select a latent space dimensionality space of $2$.  The remaining hidden layers will have sizes $112$ and $16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vp)\n",
    "\n",
    "# Instantiate model\n",
    "n_epochs = 5\n",
    "n_layers = [784, 112, 16]\n",
    "n_latent = 4\n",
    "ae_model = vp.SymmetricLinearAE(n_layers, n_latent)\n",
    "ae_model = ae_model.to(device)  # apply model to device.\n",
    "\n",
    "# Select optimizer.\n",
    "lr = 1e-3\n",
    "opt = optim.Adam(ae_model.parameters(), lr=lr)\n",
    "\n",
    "# Select loss fuction.\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vp)\n",
    "# Train model.\n",
    "validation_loss = vp.train_AE(train_dl, validation_dl, ae_model, opt, loss_fn, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright --- let's see how the model performs.  \n",
    "# Note that we have to unflatten the prediction, which we do using some torch.nn functionality.\n",
    "x_test, y_test = validation_dl.dataset.__getitem__(200)\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "# We select the second dimension of the 2D output to unflatten into a 28x28 image.\n",
    "unflatten = nn.Unflatten(1, (28, 28))\n",
    "\n",
    "print(y_test)  # get encoding output. \n",
    "plt.imshow(x_test.to('cpu').permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "x_pred = ae_model(x_test.unsqueeze(0))\n",
    "# Call the unflatten object.\n",
    "x_pred = unflatten(x_pred)\n",
    "\n",
    "plt.imshow(x_pred.detach().to('cpu').permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty neat result thus far.  Let's consider a couple things real quick.  First, this is only a four-layer encoder that maps to the latent space $\\mathcal{Z}$.  We could increase the network depth, or we could even add convolutional layers --- after all, we are trying to encode images.  Second, this encoding exists in two dimensions: $\\mathbf{z}\\in \\mathcal{R}^2$.  This means we were able to start with $768$-dimension vectors and then find a two-dimensional representation that contains enought information about the training data to then recover the initial image with impressive accuracy.  \n",
    "\n",
    "Let's actually make use of this latent representation and run some clustering algorithms on the validation data set's encoded representations.  Before doing so, we'll collect the encoding predictions first, and the plot the bivariate encoding to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's loop the validation data set through and get images and labels.\n",
    "encoding_batches, labels_batches = zip(\n",
    "    *[(ae_model.get_encoding(xb.to(device)), yb.to(device)) for xb, yb in validation_dl]\n",
    ")\n",
    "\n",
    "# Take the tuples of batches and concatenate.  Also, detach the tensors --- no need to track gradients.\n",
    "encodings = concat(encoding_batches).detach().cpu().numpy()\n",
    "# normed_encodings = vp.SymmetricLinearAE.normalize_encoding(encodings)\n",
    "labels = concat(labels_batches).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this 2D encoding look like?\n",
    "encoding_fig = ae_plots.plot_2Dencoding_predictions(\n",
    "    encodings, figsize=(8, 8), marker='.', color='black', alpha=0.2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data exhibits some clearly nonlinear behavior.  Regardless, let's play around with some classification algorithms and see how things look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn models must first be instantiated, then fit using the 'fit()' method.  \n",
    "# Keep in mind that the fitting sample must be fed in as a column vector of features. \n",
    "# Because we are fitting clustering algorithms, no labels (aka no independent variable values)\n",
    "# Need to be provided (specifically, the keyword argument 'y=None' for 'fit()').\n",
    "\n",
    "# Instantiate and fit a K-Means model with 10 clusters.\n",
    "km_model = KMeans(init='k-means++', n_clusters=10, n_init=8)\n",
    "km_model.fit(encodings)\n",
    "\n",
    "# Instantiate and fit a Spectral Clustering model with 10 clusters. This\n",
    "# algorithm is slower than k-means but does not assume densities are convex\n",
    "# or isotropic.\n",
    "sc_model = SpectralClustering(n_clusters=10, n_init=8)\n",
    "sc_model.fit(encodings)\n",
    "\n",
    "# Instantiate and fit a DBSCAN model.  This algorithm is more unsupervised,\n",
    "# finding the label count from the data.\n",
    "hds_model = HDBSCAN(min_cluster_size=200, min_samples=50)\n",
    "hds_model.fit(encodings)\n",
    "\n",
    "# Instantiate and fit a DBSCAN model.  This algorithm is more unsupervised,\n",
    "# finding the label count from the data.\n",
    "# bw = estimate_bandwidth(encodings, n_samples=500)\n",
    "ms_model = MeanShift(bandwidth=0.1)\n",
    "ms_model.fit(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the different clustering algorithm results.\n",
    "km_fig = ae_plots.plot_2Dencodings_withclusters(\n",
    "    encodings, km_model.labels_,\n",
    "    'K-Means', 10,\n",
    "    encoding_kwargs={'color': 'black', 'marker': '.', 'alpha': 0.2},\n",
    "    clusters_kwargs={'marker': 'o', 'alpha': 0.15}\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sc_fig = ae_plots.plot_2Dencodings_withclusters(\n",
    "    encodings, sc_model.labels_,\n",
    "    'Spectral Clustering', 10,\n",
    "    encoding_kwargs={'color': 'black', 'marker': '.', 'alpha': 0.2},\n",
    "    clusters_kwargs={'marker': 'o', 'alpha': 0.15}\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "hds_fig = ae_plots.plot_2Dencodings_withclusters(\n",
    "    encodings, hds_model.labels_,\n",
    "    'HDBSCAN', hds_model.n_features_in_,\n",
    "    encoding_kwargs={'color': 'black', 'marker': '.', 'alpha': 0.2},\n",
    "    clusters_kwargs={'marker': 'o', 'alpha': 0.15}\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "ms_fig = ae_plots.plot_2Dencodings_withclusters(\n",
    "    encodings, ms_model.labels_,\n",
    "    'Mean Shift', ms_model.n_features_in_,\n",
    "    encoding_kwargs={'color': 'black', 'marker': '.', 'alpha': 0.2},\n",
    "    clusters_kwargs={'marker': 'o', 'alpha': 0.15}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2.2: ConvNet Autoencoder.\n",
    "\n",
    "Already we are running into issues discovering clusters in the encoded representation.  The encoded representation is not well-bahaved, featuring complex desnity curvature including varying length scales.  That alone hints that our encoding may not be very good, even if the decoder can recover a decent prediction from the latent space representation.  It could very well be that only having two latent features is simply insufficient to describe variation in our training set.  \n",
    "\n",
    "Recall that we are trying to find a latent representation of image data --- maybe it makes sense to instead build an autoencoder with ConvNet layers. Unfortunately, this is easier said then done, at least if we want our latent space to be two dimensional.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3]), (3,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import rand\n",
    "s = (3,)\n",
    "x = rand(s)\n",
    "s == x.shape, x.shape, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
