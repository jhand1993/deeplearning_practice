{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44a45a2-f228-4e60-b8a3-e25d385b789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import cuda, tensor, nn, optim\n",
    "from torch.backends import mps\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "import datasets.catdog_loader as catdog_loader\n",
    "from ptmodels import bic_pytorch as bp\n",
    "\n",
    "# Use a gpu or M1 chipset to train PyTorch networks if you have it.\n",
    "if cuda.is_available():\n",
    "    pt_device = 'cuda'\n",
    "\n",
    "elif mps.is_available():\n",
    "    pt_device = 'mps'\n",
    "\n",
    "else: \n",
    "    pt_device = 'cpu'\n",
    "\n",
    "print(f'Using {pt_device}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa830ab0-4f8d-4bc2-a20a-fdc5573907eb",
   "metadata": {},
   "source": [
    "# Project One: Dog and Cat Classifier\n",
    "The goal of this project is to use a kaggle dataset to train a dog and cat classifier.  The data consists of user images.  Because this is binary classification, we can probably get awayout one-hot encoding: $y(\\mathbf{x}=f(\\mathbf{x})$, where $\\mathbf{x}$ is either a flattened image vector or an 2D image matrix and $f$ is the function we are trying to model with $y$ being the predicted label.  \n",
    "\n",
    "Classifiers will be implemented using PyTorh.  The first will be a simple logistics classifier, the second a three-layer perceptron with `relu` activation functions for the hidden layers, and finally a ConvNet with a final linear layer feeding into a sigmoid activation function. \n",
    "\n",
    "The first thing we need to do is load the data and format it for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844e002-9ab5-4fdb-8f6c-d839af075c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('datasets') / 'kagglecatsanddogs_5340' / 'PetImages'\n",
    "cat_path = data_path / 'Cat'\n",
    "dog_path = data_path / 'Dog'\n",
    "cat_img_names = list(cat_path.glob('*.jpg'))\n",
    "dog_img_names = list(dog_path.glob('*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3698e-60b6-46e1-afcb-029330a931d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up some images. \n",
    "def get_image(fname):\n",
    "    assert Path(fname).is_file()\n",
    "    return np.asarray(io.imread(fname), dtype=int)\n",
    "\n",
    "cat_test = cat_img_names[1]\n",
    "dog_test = dog_img_names[1]\n",
    "\n",
    "cat_test_img = get_image(cat_test)\n",
    "dog_test_img = get_image(dog_test)\n",
    "\n",
    "plt.imshow(cat_test_img)\n",
    "plt.show()\n",
    "plt.imshow(dog_test_img)\n",
    "plt.show()\n",
    "\n",
    "print(cat_test_img.shape, dog_test_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7c2f6",
   "metadata": {},
   "source": [
    "Before training, we want to make sure loaded images will transform appropriately into `PyTorch` `Tensor` objects.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f441c0e",
   "metadata": {},
   "source": [
    "Before training, let's move incompatible, corrupt, or bad images to two new directories: `PetImages/notused_cat` and `PetImages/notused_dog`.  It is good practice to not immediately delete problematic data -- for example, you can always come back to it and see if you can transform incompatible file types to increase your training set if you need to!  Remember that doing this once will make irreversible changes. \n",
    "\n",
    "Note: we are creating a `torchvision.transforms.ToTensor()` instance to make sure the loaded image arrays properly convert to `torch.Tensor` objects.  More on that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are just going to script this workflow since it is a one-time thing. \n",
    "\n",
    "notused_cat_path = data_path / 'notused_cat'\n",
    "notused_dog_path = data_path / 'notused_dog'\n",
    "\n",
    "notused_cat_path.mkdir(exist_ok=True)\n",
    "notused_dog_path.mkdir(exist_ok=True)\n",
    "\n",
    "dont_use_cat = []\n",
    "for f in cat_path.glob('*.jpg'):\n",
    "    try:\n",
    "        # First, we load an image and see if it fails.  If it does,\n",
    "        # then we add it to the don't use list. \n",
    "        arr = io.imread(str(f))\n",
    "        arr = transforms.ToTensor()(arr)\n",
    "\n",
    "\n",
    "        # Also, if there are not three color channels, we also are \n",
    "        # going to move the file. Note that torchvision puts channels \n",
    "        # first.\n",
    "        if arr.shape[0] != 3:\n",
    "            dont_use_cat.append(f)\n",
    "    \n",
    "    except:\n",
    "        print(f'{f} raised exception when read.  Skipping...')\n",
    "        dont_use_cat.append(f)\n",
    "\n",
    "\n",
    "dont_use_dog = []\n",
    "for f in dog_path.glob('*.jpg'):\n",
    "    try:\n",
    "        # First, we load an image and see if it fails.  If it does,\n",
    "        # then we add it to the don't use list. \n",
    "        arr = io.imread(str(f))\n",
    "        arr = transforms.ToTensor()(arr)\n",
    "\n",
    "        # Also, if there are not three color channels, we also are \n",
    "        # going to move the file. Note that torchvision puts channels \n",
    "        # first.\n",
    "        if arr.shape[0] != 3:\n",
    "            dont_use_dog.append(f)\n",
    "    \n",
    "    except:        \n",
    "        print(f'{f} raised exception when read.  Skipping...')\n",
    "        dont_use_dog.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15622a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dont_use_cat), len(dont_use_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92356c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can take a minute to go through all the image files.  Now we move them \n",
    "# to the new 'not used' directories.\n",
    "for f in dont_use_cat:\n",
    "    new_path = notused_cat_path / f.name\n",
    "    shutil.copy(f, new_path)\n",
    "    f.unlink() # easy way to remove a file located at the path 'f'. \n",
    "\n",
    "for f in dont_use_dog:\n",
    "    new_path = notused_dog_path / f.name\n",
    "    shutil.copy(f, new_path)\n",
    "    f.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27a269",
   "metadata": {},
   "source": [
    "We have saved troublesome images to look at later if we are so obliged. Let's come back to loading the data set.  As mentioned, we are running into some design issues in our current implementation. Instead of implementing this functionality nearly from scratch, let's use PyTorch's DataSet and DataLoader classes.  The file `catdog_loader.py` contains a DataSet subclass `CatDogDataSet` which we can use to load up the data set. Note that `CatDogDataSet` using `scikit-image` instead of `torchvision` for file io --- numpy arrays are then transformed to `torch.tensor` opjects using `torchvision.ToTensor()`, which conveniently reformats image tensors to be channel-first (the PyTorch de facto standard) and normalizes counts from $(0,255)$ to $(0,1)$ for training. \n",
    "\n",
    "We can also feed `torchvision.transforms` transformations to chain image transformations together for our input image Tensors and outputs.  First, we are gonig to want to resize the images to be all the same input size --- specifically, we'll resize each image to $128\\times128$ pixels per color channel.  This means each image will be a $3\\times128\\times128$ tensor.  Optionally, we code also convert to one-hot encoding (see commented out code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(catdog_loader)\n",
    "pt_transforms = transforms.Compose(\n",
    "    [transforms.Resize((128, 128), antialias=True),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "data_pt = catdog_loader.CatDogDataSet(\n",
    "    data_path, transform=pt_transforms\n",
    ")\n",
    "\n",
    "# If we want one-hot encoding, we could do this using the 'target_transform' kwarg.  Just make sure \n",
    "# to specify the number of classes for the encoding. A lambda function will work well to do that here. \n",
    "# data_pt = catdog_loader.CatDogDataSet(\n",
    "#     data_path, transform=pt_transforms, target_transform=lambda x: nn.functional.one_hot(x, num_classes=2)\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe86cf",
   "metadata": {},
   "source": [
    "Already, we see the advantage of not reinventing the wheel.  PyTorch and torchvision provide powerful tools to pre-process, load, and transform data.  Unless otherwise noted, they are tried modules with robust support.  We will stick with PyTorch IO for the remainder of this notebook. \n",
    "\n",
    "Next, we want to prepare the data set to be fed forward through the networks we will be using.  First, we are going to split the data into random subsamples --- one for training, the other for validating. Again, PyTorch makes this easy by providing an easy way to do this with `torch.utils.data.random_split`.  After that, we are going to prepare the data for network feed forward by using the `DataLoader` class.  It makes it easy to create random batches of specified size from a `DataSet` object (specifically, the `CatDogDataSet` object we wrote).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = random_split(data_pt, [0.8, 0.2])\n",
    "dataload_train = DataLoader(data_train, batch_size=128, shuffle=True)\n",
    "dataload_valid = DataLoader(data_valid, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data_train.__getitem__(0)\n",
    "x.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922efe1c",
   "metadata": {},
   "source": [
    "We are set to start training some networks.  Let's start with a PyTorch implementation of a single layer linear network with a sigmoid activation function.  This is equivalent to logistics classification.  Any neural network training workflow requires we specify a model, a loss function, and an optimizer.  We'll do that now, along with specifying some hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(bp)\n",
    "# Number of epochs and learning rate for optimizer. \n",
    "n_epochs = 5\n",
    "lr = 0.0005\n",
    "\n",
    "# When instantiating the PyTorch logistic classifier model, we can also tell it to use \n",
    "# the specified device to train.  If you have a GPU, this is an easy way to use it.\n",
    "pt_logistic = bp.Logistic(n_in=128 * 128 * 3).to(pt_device)\n",
    "\n",
    "# Since this is a binary classification problem, we use a binary cross entropy loss function. \n",
    "# Note that 'binary_cross_entropy' is a function, so don't add () at the end here!\n",
    "pt_log_loss = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "# The optimizer will always need the model parameters as input.  This makes sense --- \n",
    "# you can't optimize a model without changing the model parameters!\n",
    "pt_log_opt = optim.Adam(pt_logistic.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec35e7",
   "metadata": {},
   "source": [
    "One thing to note here --- we are feeding `pt_device` into the training loop function `train_bic_model`.  This is important, because we need to run ALL tensors on the same device, be it a GPU via CUDA or just a default CPU.  Check out the function definition in `bic_pytorch.py` to see where the tensors being set to run on the given device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d500ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.reset_params(pt_logistic)\n",
    "\n",
    "v_loss = bp.train_bic_model(dataload_train, dataload_valid, pt_logistic, pt_log_opt, pt_log_loss, n_epochs, pt_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b267a",
   "metadata": {},
   "source": [
    "We've trained a logistics binary image classifier.  Mathematically, this is a not a sophisticated implementation.  Our model is\n",
    "\n",
    "$z_i = \\sum^{N_{pixels}}_j w_jx_j + b_i$, \n",
    "\n",
    "$y_i = \\frac{1}{1 + \\exp(-z_i)}$\n",
    "\n",
    "where $w_j$ is the $j$-th weight (or node) parameter for the $j$-th pixel $x_j$, and $b_i$ is the bias.  $y_i$ is the predicted label, which should be $1$ for cats or $0$ for dogs with a perfect model.  $z_i$ is the unnormalized probability prediction called logits. \n",
    "\n",
    "A multilayer perceptron can be thought as layered logistic models before feeding logits into the sigmoid function.  Between each layer, and activation function imparts nonlinearity to the model, allowing for the perceptron model to learn more complex behavior than the logistics model above.  Lets have $128 \\times 128 \\times 3$ nodes for layer 1, $16 \\times 16 \\times 3$ nodes for layer 2, and $2 \\times 2 \\times 3$ for layer 3. That finally layer's logits will then be fed into a sigmoid function to provide predicted label probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(bp)\n",
    "# Number of epochs and learning rate for optimizer. \n",
    "n_epochs = 5\n",
    "lr = 0.0005\n",
    "\n",
    "# When instantiating a three-layer perception model.\n",
    "n_layers = (49152, 786, 24) # Each layer will become progressively sparse. \n",
    "pt_perceptron = bp.Perceptron(n_layers=n_layers).to(pt_device)\n",
    "\n",
    "# Since this is a binary classification problem, we use a binary cross entropy loss function. \n",
    "# Note that 'binary_cross_entropy' is a function, so don't add () at the end here!\n",
    "pt_perp_loss = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "# The optimizer will always need the model parameters as input.  This makes sense --- \n",
    "# you can't optimize a model without changing the model parameters!\n",
    "pt_perp_opt = optim.Adam(pt_perceptron.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c721dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.reset_params(pt_perceptron)\n",
    "\n",
    "v_loss = bp.train_bic_model(dataload_train, dataload_valid, pt_perceptron, pt_perp_opt, pt_perp_loss, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1c754",
   "metadata": {},
   "source": [
    "So far, so good.  Now, let's implement a Convolutional deep neural network.  ConvNets are useful because they structurally account for spatial correlation and invariances intrinsic to images.  They also require significantly less resources than linear models to achieve similar performance.  \n",
    "\n",
    "The network we will be using will use kernel sizes for convolution as specified by the user: `k1` and `k2` for ConvNet layers 1 and 2, respectively.  These kernels will increase channel counts by factors of 2 as well.  We will pad the inputs to these ConvNets so that image size is preserved. After each ConvNet layer, we feed the kernel weights through a nonlinear ReLU activation function.  After that, max pool layers will reduce input image sizes by a factor of four. After two ConvNet layers with their respective activation and pooling, the output will be flattened. The original $128\\times 128\\times 3$ image will be reduced by a factor $(4\\times4)^2$ due to pooling, and increased by a factor of $2^2$ due to channel count increase, leaving a flattened vector of length $8\\times 8\\times 12=768$. That vector will then go through two linear layers with another ReLU layer in between, finally outputing a scalar logits value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(bp)\n",
    "# Number of epochs and learning rate for optimizer. \n",
    "n_epochs = 5\n",
    "lr = 0.0005\n",
    "\n",
    "# When instantiating a three-layer perception model.\n",
    "# square kernel sizes. \n",
    "k1 = 7\n",
    "k2 = 3\n",
    "n3 = 128 # input size for the final linear layer.\n",
    "pt_conv = bp.ConvNet(k1, k2, n3, pad='same').to(pt_device)\n",
    "\n",
    "# Since this is a binary classification problem, we use a binary cross entropy loss function. \n",
    "# Note that 'binary_cross_entropy' is a function, so don't add () at the end here!\n",
    "pt_conv_loss = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "# The optimizer will always need the model parameters as input.  This makes sense --- \n",
    "# you can't optimize a model without changing the model parameters!\n",
    "pt_conv_opt = optim.Adam(pt_conv.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5baa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.reset_params(pt_conv)\n",
    "\n",
    "v_loss = bp.train_bic_model(dataload_train, dataload_valid, pt_conv, pt_conv_opt, pt_conv_loss, n_epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51ef27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
